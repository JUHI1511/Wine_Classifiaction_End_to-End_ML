# -*- coding: utf-8 -*-
"""winedataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fccc4QSfT_CiANGZtjlqiX505faQZrsx
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

# %matplotlib inline

dataset=pd.read_csv('/content/winequality_red.csv')
print(dataset.head())

x=dataset.iloc[:,:-1].values
y=dataset.iloc[:,-1].values

print(x)

print(y)

dataset.info()

dataset.describe()

dataset.isnull().sum()

dataset.columns

fig=plt.figure(figsize=(10,5))
sns.barplot(x='quality',y='fixed acidity',data=dataset)

plt.figure(figsize=(10,6))
sns.barplot(x='quality',y='volatile acidity',data=dataset)

plt.figure(figsize=(10,6))
sns.barplot(x='quality',y='citric acid',data=dataset)

plt.figure(figsize=(10,6))
sns.barplot(x='quality',y='residual sugar',data=dataset)

plt.figure(figsize=(10,6))
sns.barplot(x='quality',y='chlorides',data=dataset)

plt.figure(figsize=(10,6))
sns.barplot(x='quality',y='density',data=dataset)

plt.figure(figsize=(10,6))
sns.barplot(x='quality',y='alcohol',data=dataset)

bins=(3,6.5,8)
group_names=['bad','good']
dataset['quality']=pd.cut(dataset['quality'],bins=bins,labels=group_names)

dataset.head()

print(y)

type(y)

from sklearn.impute import SimpleImputer
imputer=SimpleImputer(missing_values=np.nan,strategy='mean')
imputer.fit(x[:,:])
x[:,:]=imputer.transform(x[:,:])

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
dataset['quality']=le.fit_transform(dataset['quality'].astype(str))

print(dataset.head())

print(y)

sns.countplot(dataset['quality'])

print(dataset['quality'].value_counts())

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=1)

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.transform(x_test)

from xgboost import XGBClassifier
c=XGBClassifier()
c.fit(x_train,y_train)

from sklearn.metrics import confusion_matrix,accuracy_score
y_pred=c.predict(x_test)
cm=confusion_matrix(y_test,y_pred)
print(cm)
print(accuracy_score(y_test,y_pred))

from sklearn.naive_bayes import GaussianNB
gn=GaussianNB()
gn.fit(x_train,y_train)

from sklearn.metrics import confusion_matrix,accuracy_score
y_gb=gn.predict(x_test)
cm=confusion_matrix(y_test,y_gb)
print(cm)
print(accuracy_score(y_test,y_gb))

from sklearn.tree import DecisionTreeClassifier
ds=DecisionTreeClassifier(criterion='entropy',random_state=5)
ds.fit(x_train,y_train)

from sklearn.metrics import confusion_matrix,accuracy_score
y_dtc=ds.predict(x_test)
print(confusion_matrix(y_test,y_dtc))
print(accuracy_score(y_test,y_dtc))

from sklearn.ensemble import RandomForestClassifier
rc=RandomForestClassifier(n_estimators=136,criterion='entropy',random_state=1)
rc.fit(x_train,y_train)

from sklearn.metrics import confusion_matrix,accuracy_score
y_rfc=rc.predict(x_test)
print(confusion_matrix(y_test,y_rfc))
print(accuracy_score(y_test,y_rfc))

from sklearn.neighbors import KNeighborsClassifier

knn=KNeighborsClassifier(n_neighbors=10, metric='minkowski',p=2)
knn.fit(x_train,y_train)
y_knn=rc.predict(x_test)

from sklearn.metrics import confusion_matrix,accuracy_score
y_knn=knn.predict(x_test)
print(confusion_matrix(y_test,y_knn))
print(accuracy_score(y_test,y_knn))

from sklearn.svm import SVC
svm=SVC(kernel='rbf',random_state=0)
svm.fit(x_train,y_train)

from sklearn.metrics import confusion_matrix,accuracy_score
y_svm=svm.predict(x_test)
print(confusion_matrix(y_test,y_svm))
print(accuracy_score(y_test,y_svm))

from sklearn.model_selection import cross_val_score
accuracies=cross_val_score(estimator=svm,X=x_train,y=y_train,cv=10)
print('Accuracy: {:.2f} %'.format(accuracies.mean()*100))
print('standerd Division: {:.2f} %'.format(accuracies.std()*100))

from sklearn.model_selection import GridSearchCV
parameters={'n_estimators':[100,200],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [4,5,6,7,8],
    'criterion' :['gini', 'entropy']}
grid_search=GridSearchCV(estimator=rc,param_grid=parameters,cv=5)
grid_search.fit(x_train,y_train)
best_accuracy=grid_search.best_score_
best_parameters=grid_search.best_params_
print('Best Accuracy: {:.2f} %'.format(best_accuracy.mean()*100))
print('Best parameters: ',best_parameters)

from xgboost import XGBClassifier
import pickle

pickle.dump(rc,open('model.pkl','wb'))
wine_classification_model=pickle.load(open('model.pkl','rb'))
y_pred=wine_classification_model.predict(x_test)
print(confusion_matrix(y_test,y_pred))
print(accuracy_score(y_test,y_pred))